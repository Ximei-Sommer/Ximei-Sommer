<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1a41d4b8-0d39-801d-a86b-c8e549a41f23" class="page sans"><header><img class="page-cover-image" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/luna.png" style="object-position:center 8.57%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">🌕</span></div><h1 class="page-title">Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction</h1><p class="page-description"></p></header><div class="page-body"><p id="1a41d4b8-0d39-803b-b18d-f73cfe17d6c1" class="">FEBRUARY 25, 2025</p><p id="1a41d4b8-0d39-8017-999c-e0e2bdfc7843" class="block-color-gray"><strong>Ximei Xu, ximeixu79@gmail.com</strong></p><p id="1a41d4b8-0d39-8023-881b-e1e9d2550fc7" class="">
</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1a41d4b8-0d39-8068-a14e-d92ef8a0fdec"><div style="font-size:1.5em"><span class="icon">🐣</span></div><div style="width:100%"><p id="1a41d4b8-0d39-8055-88c5-db47421369f6" class="">In this blog, I will introduce the paper <em>&quot;Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction&quot;.</em> This paper presents <strong>Extract-Define-Canonicalize (EDC)</strong>, an LLM-based three-phase framework that addresses the problem of knowledge graph construction by open information extraction followed by post-hoc canonicalization.</p></div></figure><h1 id="1a41d4b8-0d39-80ab-9d57-f212baf57fe3" class="">Introduction &amp; Background</h1><p id="1a41d4b8-0d39-805c-8062-f9eacdbf63fd" class="">This paper proposes an approach to constructing knowledge graphs that leverages <strong>large language models (LLMs)</strong> in a structured manner.</p><h2 id="1a41d4b8-0d39-802d-aa7c-df4afc42c7d8" class="">Knowledge Graph Construction</h2><ul id="1a41d4b8-0d39-80f0-982a-f74fb343a29a" class="bulleted-list"><li style="list-style-type:disc">Traditional methods typically addressed <strong>knowledge graph construction (KGC)</strong> using “<strong>pipelines</strong>”, comprising subtasks like entity discovery, entity typing, and relation classification. </li></ul><ul id="1a41d4b8-0d39-8015-aa32-df8a37b649f4" class="bulleted-list"><li style="list-style-type:disc">Thanks to advances in <strong>pre-trained generative language models</strong> (e.g., T5 and BERT), more recent works instead frame KGC as a <strong>sequence-to-sequence</strong> problem and generate relational triplets in an <strong>end-to-end</strong> manner by <strong>fine-tuning</strong> these moderately-sized language models. </li></ul><ul id="1a41d4b8-0d39-8016-a96e-d38091544600" class="bulleted-list"><li style="list-style-type:disc">The success of <strong>large language models (LLMs)</strong> has pushed this paradigm further: current methods directly prompt the LLMs to generate triplets in a <strong>zero/few-shot</strong> manner. <ul id="1a41d4b8-0d39-80a6-a4be-f71dda0773f6" class="bulleted-list"><li style="list-style-type:circle"><strong>Problem:</strong> These models face difficulties scaling up to <strong>general text</strong> common in many real-world applications <strong>as</strong> the KG schema has to be included in the LLM prompt. </li></ul></li></ul><ul id="1a41d4b8-0d39-8094-a116-c0fdd3dff519" class="bulleted-list"><li style="list-style-type:disc">The <strong>Extract-Define-Canonicalize (EDC)</strong> framework proposed in this paper circumvents this problem by using <strong>post-hoc canonicalization</strong> (and <strong>without</strong> requiring fine-tuning of the base LLMs).</li></ul><h2 id="1a41d4b8-0d39-8037-af4b-dbbe4ec91d81" class="">Open Information Extraction and Canonicalization</h2><p id="1a41d4b8-0d39-80d7-8641-db6c429809eb" class=""><strong>Standard (closed) information extraction:</strong> It requires the output triplets to follow a pre-defined schema, e.g. a list of relation or entity types to be extracted from. </p><p id="1a41d4b8-0d39-80bb-b391-d1cb99acc629" class=""><strong>Open information extraction (OIE):</strong> It does not have such a requirement.</p><p id="1a41d4b8-0d39-8071-ac34-dc48af22c4d0" class="">Recent studies have found LLMs to exhibit excellent performance on OIE tasks. <div class="indented"><ul id="1a41d4b8-0d39-8088-800e-cfcc6fc56a4d" class="bulleted-list"><li style="list-style-type:disc"><strong>Problem:</strong> The relational triplets extracted from OIE systems are not canonicalized, causing redundancy and ambiguity in the induced open knowledge graph. </li></ul><ul id="1a41d4b8-0d39-807c-bca9-c2c75c8106d2" class="bulleted-list"><li style="list-style-type:disc"><strong>Current solutions:</strong> An extra canonicalization step is required to standardize the triplets. <ul id="1a41d4b8-0d39-80d6-b2b1-ce86933f0f29" class="bulleted-list"><li style="list-style-type:circle">In case a target schema is present: “Alignment”.  </li></ul><ul id="1a41d4b8-0d39-80a7-9be3-d2f30eafc59a" class="bulleted-list"><li style="list-style-type:circle">In case no target schema is available: Clustering. <ul id="1a41d4b8-0d39-8054-a769-e4d1462b723f" class="bulleted-list"><li style="list-style-type:square">However, clustering-based methods are prone to over-generalization, e.g., it may put “is brother of”, “is son of”, “is main villain of”, and “was professor of” into the same relation cluster.</li></ul></li></ul></li></ul></div></p><p id="1a41d4b8-0d39-80eb-90b1-c2f4f8a4724b" class=""><strong>Extract-Define-Canonicalize (EDC):</strong> It is more general compared to the existing canonicalization methods. </p><ul id="1a41d4b8-0d39-8074-9981-c4606bd3ca7c" class="bulleted-list"><li style="list-style-type:disc">EDC works whether a target schema is provided or not. </li></ul><ul id="1a41d4b8-0d39-80ce-8e26-e5e098485706" class="bulleted-list"><li style="list-style-type:disc">EDC alleviates the over-generalization issue, by allowing the <strong>LLMs</strong> to <strong>verify</strong> if a transformation can be performed (instead of solely relying on the embedding similarity).</li></ul><h1 id="1a41d4b8-0d39-8027-ae66-e8efc1c5d19e" class="">Method</h1><p id="1a41d4b8-0d39-80b3-aa91-d681a07d0d65" class="">The proposed LLM-based <strong>three-phase</strong> framework is called <strong>Extract-Define-Canonicalize (EDC)</strong>: open information extraction followed by schema definition and post-hoc canonicalization. Given input text, the goal here is to extract <strong>relational triplets</strong> in <strong>a canonical form</strong> such that the resulting KGs will have minimal ambiguity and redundancy. When there is a <strong>predefined target schema</strong>, all generated triplets should conform to it. In the scenario where there is not one, the system should dynamically create one and canonicalize the triplets with respect to it. Figure 1 shows an overview of the framework.</p><figure id="1a41d4b8-0d39-80c1-b581-c7299e5f50fe" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/main_fig.png"><img style="width:709.9910888671875px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/main_fig.png"/></a><figcaption>Figure 1: A high-level illustration of Extract-Define-Canonicalize (EDC) for Knowledge Graph Construction.</figcaption></figure><p id="1a41d4b8-0d39-8004-890d-d229f2b261b6" class="">In this section, I will first introduce the EDC framework followed by a description of refinement (EDC+R). </p><h2 id="1a41d4b8-0d39-8088-95c5-fb4d9af59080" class="">EDC: Extract-Define-Canonicalize</h2><p id="1a51d4b8-0d39-804b-a5ba-dd6abcae4104" class="">At a high level, EDC decomposes KGC into <strong>three</strong> connected subtasks. </p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a51d4b8-0d39-802a-83e2-edc241bec750"><div style="font-size:1.5em"><span class="icon">👨🏼‍🚀</span></div><div style="width:100%"><p id="1a51d4b8-0d39-8006-a9b6-c83d95f4d9e5" class="">In the following discussion, a specific input text example will be used for better illustration: “Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew.”</p></div></figure><h3 id="1a51d4b8-0d39-80d7-b6ce-d0e996f8ee33" class="">Phase 1: Open Information Extraction</h3><p id="1a51d4b8-0d39-8022-bc99-cd0fd61a8a3d" class="">Firstly Large Language Models (LLMs) are leveraged for open information extraction. Through <strong>few-shot</strong> prompting, LLMs identify and extract relational triplets ([Subject, Relation, Object]) from input texts, <strong>independent</strong> of any specific schema. Using the example above, the prompt is:</p><figure id="1a51d4b8-0d39-8048-ade3-f87148b5790c" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/OIE.png"><img style="width:492.0000305175781px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/OIE.png"/></a><figcaption>Figure 2:  An example of OIE Prompt.</figcaption></figure><p id="1a51d4b8-0d39-80de-b0c6-e537bab72e27" class="">The resultant triplets form an <strong>open KG</strong>, which is forwarded to subsequent phases.</p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a51d4b8-0d39-80c2-b157-d25fd8318d7e"><div style="font-size:1.5em"><span class="icon">👨🏼‍🚀</span></div><div style="width:100%"><p id="1a51d4b8-0d39-8054-9633-f9411f2a42a7" class="">In this case, the resultant triplets are [‘Alan Shepard’, ‘bornOn’, ‘Nov 18, 1923’], [‘Alan Shepard’, ‘participatedIn’, ‘Apollo 14’].</p></div></figure><h3 id="1a51d4b8-0d39-80d7-b8b3-e2f5389fdc5b" class="">Phase 2: Schema Definition</h3><p id="1a51d4b8-0d39-80c4-8e94-c43976e31261" class="">Next, the LLMs are prompted to provide a <strong>natural language definition</strong> for each component of the schema induced by the <strong>open KG</strong>:</p><figure id="1a51d4b8-0d39-8044-8444-c949afc3d30c" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/SD1.png"><img style="width:709.982177734375px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/SD1.png"/></a><figcaption>Figure 3:  An example of Schema Definition Prompt.</figcaption></figure><p id="1a51d4b8-0d39-8070-a61a-f43a61e0ae71" class="">The resultant <strong>definitions</strong> for the schema components are then passed to the next stage as <strong>side information</strong> used for <strong>canonicalization</strong>.</p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a51d4b8-0d39-8003-8154-df69d332ddb5"><div style="font-size:1.5em"><span class="icon">👨🏼‍🚀</span></div><div style="width:100%"><p id="1a51d4b8-0d39-801f-846c-e40e2d251dca" class="">This example prompt results in the definitions for (<strong>bornOn</strong>: The subject entity was born on the date specified by the object entity.) and (<strong>participatedIn</strong>: The subject entity took part in the event or mission specified by the object entity.).</p></div></figure><h3 id="1a51d4b8-0d39-8076-bbba-db0c088ad694" class="">Phase 3: Schema Canonicalization </h3><p id="1a51d4b8-0d39-8005-9e21-c467bcefa6cd" class="">The third phase aims to refine the open KG into <strong>a canonical form</strong>, eliminating redundancies and ambiguities. It starts by <strong>vectorizing</strong> the <strong>definitions</strong> of each schema component using a <strong>sentence transformer</strong> to create embeddings. Canonicalization then proceeds in one of two ways, depending on the availability of a <strong>target</strong> schema:</p><ul id="1a51d4b8-0d39-805a-88ed-f41ada29e998" class="bulleted-list"><li style="list-style-type:disc"><strong>Target Alignment:</strong> With an existing target schema, the goal is to identify <strong>the most closely related</strong> components within the target schema for each element, considering them for canonicalization. To prevent issues of<strong> over-generalization</strong>, LLMs assess the <strong>feasibility</strong> of each potential transformation. If a transformation is deemed <strong>unreasonable</strong>, the component and its related triplets are <strong>excluded</strong>.</li></ul><ul id="1a51d4b8-0d39-80da-8e5f-fa8fb7837446" class="bulleted-list"><li style="list-style-type:disc"><strong>Self Canonicalization</strong>: Absent a target schema, the goal is to consolidate <strong>semantically similar</strong> schema components, standardizing them to <strong>a singular representation</strong> to streamline the KG. Starting with an <strong>empty</strong> canonical schema, it examines the open KG triplets, searching for potential consolidation candidates through <strong>vector similarity</strong> and <strong>LLM verification</strong>. Unlike target alignment, components deemed <strong>non-transformable</strong> are <strong>added</strong> to the canonical schema, thereby expanding it.</li></ul><p id="1a51d4b8-0d39-80b5-a561-c61102cf4a11" class="">The prompt for the example is:</p><figure id="1a51d4b8-0d39-809c-bc1c-f9d83b3bab76" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/SC1.png"><img style="width:709.9732666015625px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/SC1.png"/></a><figcaption>Figure 4:  An example of Schema Canonicalization Prompt.</figcaption></figure><p id="1a51d4b8-0d39-8022-b90f-d171758221ff" class="">Note that the choices above are obtained by using <strong>vector similarity search</strong>. After the LLM makes<br/>its choice, the relations are transformed to yield new triplets, which forms <br/><strong>the canonicalized KG.</strong></p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a51d4b8-0d39-8078-b2fc-ed59db3912af"><div style="font-size:1.5em"><span class="icon">👨🏼‍🚀</span></div><div style="width:100%"><p id="1a51d4b8-0d39-80eb-b3a4-fbf5aad10d50" class="">In the example, the new triplets are [‘Alan Shepard’, ‘birthDate’, ‘Nov 18, 1923’], [‘Alan Shepard’, ‘mission’, ‘Apollo 14’].</p></div></figure><h2 id="1a51d4b8-0d39-80de-af3a-f135aed9256b" class="">EDC+R: Iteratively refine EDC with Schema Retriever</h2><p id="1a51d4b8-0d39-80e0-9294-df4366483909" class="">The refinement process leverages <strong>the data generated by EDC</strong> to enhance the quality of the extracted triplets. The authors construct a “<strong>hint</strong>” for the extraction phase, which comprises two main elements:</p><ul id="1a51d4b8-0d39-8043-8be4-eedfe4ec210c" class="bulleted-list"><li style="list-style-type:disc"><strong>Candidate Entities:</strong> The entities extracted by <strong>EDC</strong> from the previous iteration, and entities extracted from the text using the LLM;<figure id="1a51d4b8-0d39-80fd-92ee-fdcc6aee2ae6" class="image" style="text-align:center"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/EE.png"><img style="width:288px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/EE.png"/></a><figcaption>Figure 5:  An example of Entity Extraction Prompt. The resultant entities are [‘Alan Shepard’, ‘Nov 18, 1923’, ‘NASA’, ‘1959’, ‘Apollo 14’].</figcaption></figure></li></ul><ul id="1a51d4b8-0d39-800e-b7df-ecadf5581a62" class="bulleted-list"><li style="list-style-type:disc"><strong>Candidate Relations:</strong> The relations extracted by <strong>EDC</strong> from the previous cycle and relations retrieved from the pre-defined/canonicalized schema by using <strong>a trained Schema Retriever</strong>.</li></ul><figure id="1a51d4b8-0d39-809a-8d02-c4791cfc1066" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/R-OIE.png"><img style="width:709.9910888671875px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/R-OIE.png"/></a><figcaption>Figure 6:  An example of Refined OIE Prompt.</figcaption></figure><p id="1a51d4b8-0d39-8036-993b-f10564a2b49f" class="">By doing so, it can provide a richer pool of candidates for the LLM, which addresses issues where the <strong>absence</strong> of entities or relations impairs the LLM’s effectiveness; it also serves to aid the OIE by <strong>bootstrapping</strong> from the previous round.</p><p id="1a51d4b8-0d39-8031-959d-f4045a231cd5" class=""><strong>Schema Retriever:</strong> To scale EDC to large schemas, a trained Schema Retriever is employed to help <strong>search</strong> schemas efficiently. It works in a similar fashion to information retrieval methods based on <strong>vector spaces</strong>; it projects the schema components and the input text to a vector space such that <strong>cosine similarity</strong> captures the relevance between the two, i.e., how likely a schema component to be <strong>present</strong> in the input text. </p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a51d4b8-0d39-8077-9fed-cca2d6495ffd"><div style="font-size:1.5em"><span class="icon">👨🏼‍🚀</span></div><div style="width:100%"><p id="1a51d4b8-0d39-8019-9536-c77f7341f02f" class="">Back to the example, <strong>refinement with the schema retriever </strong>adds the following relation to the previous set: [‘Alan Shepard’, ‘selectedByNasa’, ‘1959’]. The relation<strong> ‘selectedByNasa’ </strong>is rather obscure but was specified in the target schema.</p></div></figure><h1 id="1a41d4b8-0d39-80e4-b042-cd905c982644" class="">Experiments and Findings </h1><h2 id="1a41d4b8-0d39-80b5-8ede-e5fd8851b973" class="">Experimental Setup</h2><ol type="1" id="1a51d4b8-0d39-8046-a815-e1848ac6e4d1" class="numbered-list" start="1"><li>Datasets<p id="1a51d4b8-0d39-80d1-ba16-e72779eb2819" class="">This study evaluates EDC using <strong>three</strong> KGC datasets:</p><ul id="1a51d4b8-0d39-8062-b9c1-f12d24a96537" class="bulleted-list"><li style="list-style-type:disc"><strong>WebNLG</strong>: It contains <strong>1165</strong> pairs of text and triplets. The schema derived from these reference triplets encompasses <strong>159</strong> unique relation types.</li></ul><ul id="1a51d4b8-0d39-80cd-a751-c83f4fbe3817" class="bulleted-list"><li style="list-style-type:disc"><strong>REBEL</strong>: The authors select a random sample of <strong>1000</strong> text-triplet pairs from it. This subset induces a schema with <strong>200</strong> distinct relation types.</li></ul><ul id="1a51d4b8-0d39-8069-8cc7-e9d8b155048b" class="bulleted-list"><li style="list-style-type:disc"><strong>Wiki-NRE</strong>: The authors sample <strong>1000</strong> text-triplet pairs from it, resulting in a schema with <strong>45</strong> unique relation types.</li></ul><p id="1a51d4b8-0d39-800b-a36d-c5aed14b76e8" class="">These datasets were chosen due to their rich variety of relation types. In the experiments, this paper focuses on extracting <strong>relations</strong> as the only schema component available across all datasets. </p></li></ol><ol type="1" id="1a51d4b8-0d39-8060-8185-d63fbea5fa24" class="numbered-list" start="2"><li>EDC Models<p id="1a51d4b8-0d39-8037-a0b3-ecf788f18e18" class="">EDC contains multiple modules that are powered by LLMs. </p><ul id="1a51d4b8-0d39-80bb-b79c-ddb791d8991e" class="bulleted-list"><li style="list-style-type:disc"><strong>OIE module:</strong> Since it is the <strong>key</strong> upstream module that determines the semantic content captured in the KG, the authors tested different LLMs of different sizes including <strong>GPT-4, GPT-3.5-turbo, and Mistral-7b</strong>.</li></ul><ul id="1a51d4b8-0d39-8039-bf7d-f858ef0bd689" class="bulleted-list"><li style="list-style-type:disc"><strong>Framework’s remaining components which required prompting:</strong> They used <strong>GPT-3.5-turbo</strong>. </li></ul><ul id="1a51d4b8-0d39-80ef-b30c-d5f6cc1af4a4" class="bulleted-list"><li style="list-style-type:disc"><strong>Canonicalization phase:</strong> The <strong>E5-Mistral-7b</strong> model was utilized for vector similarity searches <strong>without modifications</strong>.</li></ul><ul id="1a51d4b8-0d39-80f3-993a-c1ed98ef0993" class="bulleted-list"><li style="list-style-type:disc"><strong>Schema Retriever:</strong> It is a <strong>fine-tuned</strong> variant of the sentence embedding model <strong>E5-mistral-7b-instruct</strong>.</li></ul></li></ol><ol type="1" id="1a51d4b8-0d39-8038-addf-ffb9ddad876f" class="numbered-list" start="3"><li>Evaluation Criteria and Baselines<ul id="1a51d4b8-0d39-806e-a559-d26884ec7979" class="bulleted-list"><li style="list-style-type:disc">Target Alignment<p id="1a51d4b8-0d39-8045-a7f6-fbf8ff08534f" class="">The authors compare EDC and EDC+R against the <strong>specialized trained</strong> models for each of the datasets:</p><ul id="1a51d4b8-0d39-804f-80bb-fda639cfbbc6" class="bulleted-list"><li style="list-style-type:circle"><strong>REGEN</strong> is the SOTA model for <strong>WebNLG</strong>. </li></ul><ul id="1a51d4b8-0d39-80af-8768-f433aeeaebee" class="bulleted-list"><li style="list-style-type:circle"><strong>GenIE</strong> is the state-of-the-art model for <strong>REBEL</strong> and <strong>Wiki-NRE</strong>.</li></ul><p id="1a51d4b8-0d39-80db-bee6-e60b940775c1" class="">This paper uses the <strong>WEBNLG evaluation script</strong> which computes the <strong>Precision, Recall, and F1 scores</strong> for the output triplets against the ground truth in a <strong>token-based</strong> manner. Metrics based on <strong>Named Entity Evaluation </strong>were used to measure the Precision, Recall, and F1 score in <strong>three</strong> different ways.</p><ul id="1a51d4b8-0d39-80e3-9e2d-de1800642d95" class="bulleted-list"><li style="list-style-type:circle">Exact: Requires a complete match between the candidate and reference triple, disregarding the type (subject, relation, object).</li></ul><ul id="1a51d4b8-0d39-808b-9f35-d885d92cdf4c" class="bulleted-list"><li style="list-style-type:circle">Partial: Allows for at least a partial match between the candidate and reference triple, disregarding the type.</li></ul><ul id="1a51d4b8-0d39-80d6-a31a-f4af1d164997" class="bulleted-list"><li style="list-style-type:circle">Strict: Demands an exact match between the candidate and reference triplet, including the element types.</li></ul></li></ul><ul id="1a51d4b8-0d39-80b2-9df9-e0d01064f517" class="bulleted-list"><li style="list-style-type:disc">Self Canonicalization<p id="1a51d4b8-0d39-8017-bd6d-c26d80b8d06a" class="">For evaluating self-canonicalization performance, comparisons are made with:</p><ul id="1a51d4b8-0d39-8050-95fb-e83d5e7b8716" class="bulleted-list"><li style="list-style-type:circle"><strong>Baseline Open KG</strong>, which is the initial open KG output from the OIE phase.</li></ul><ul id="1a51d4b8-0d39-8008-a908-eea229fb3db4" class="bulleted-list"><li style="list-style-type:circle"><strong>CESI</strong>, recognized as a leading <strong>clustering-based</strong> approach for open KG canonicalization. By applying CESI to the open KG, the authors aim to contrast its performance against canonicalization by EDC.</li></ul><p id="1a51d4b8-0d39-8053-8d59-ef3f8b461dc6" class="">The component was evaluated <strong>manually</strong>, focusing on <strong>three</strong> key aspects that reflect the intrinsic quality of an extracted KG:</p><ul id="1a51d4b8-0d39-8014-913b-d27bc55d72ab" class="bulleted-list"><li style="list-style-type:circle">Precision: The canonicalized triplets remain correct and meaningful with respect to the text compared to the OIE triplets.</li></ul><ul id="1a51d4b8-0d39-809f-8c35-da835346f8d0" class="bulleted-list"><li style="list-style-type:circle">Conciseness: The schema’s brevity is measured by the <strong>number</strong> of relations types.</li></ul><ul id="1a51d4b8-0d39-803f-9c30-c639a65586f6" class="bulleted-list"><li style="list-style-type:circle">Redundancy: They employ a redundancy score — the <strong>average cosine similarity</strong> among each canonicalized relation and its nearest counterpart — where <strong>low</strong> scores indicate that the schema’s relations are semantically <strong>distinct</strong>.</li></ul></li></ul></li></ol><h2 id="1a51d4b8-0d39-806d-9175-c7535bcb34c4" class="">Results </h2><figure id="1a51d4b8-0d39-808b-b801-f8bdcd553e7b" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/r1.png"><img style="width:709.982177734375px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/r1.png"/></a><figcaption>Figure 7: Performance of EDC and EDC+R on WebNLG, REBEL, and Wiki-NRE datasets against baselines in the Target Alignment setting (F1 scores with ‘Partial’ criteria). EDC+R only performs one iteration of refinement due to diminishing marginal improvement.</figcaption></figure><h3 id="1a51d4b8-0d39-807e-80cb-c54700c2598f" class="">Target Alignment</h3><p id="1a51d4b8-0d39-8049-b9ad-e4e3e58bded4" class="">The bar charts in Figure 7 summarize the Partial F1 scores obtained by EDC and EDC+R on all three datasets with different LLMs for OIE compared against the respective baselines. EDC demonstrates performance that <strong>is superior to or on par with</strong> the state-of-the-art baselines for all evaluated datasets. Comparing the LLMs, <strong>GPT-4</strong> emerges as the top performer, with Mistral-7b and GPT-3.5-turbo exhibiting comparable results.</p><ul id="1a51d4b8-0d39-8065-bb51-c2ada5efd2ce" class="bulleted-list"><li style="list-style-type:disc"><strong>Refinement (EDC+R) consistently and significantly enhances performance.</strong> <ul id="1a51d4b8-0d39-8068-b634-da29454d5841" class="bulleted-list"><li style="list-style-type:circle">Post-refinement, the difference in performance between <strong>GPT-3.5-turbo</strong> and <strong>Mistral-7b</strong> is larger, suggesting <strong>Mistral-7b</strong> was not as able to leverage the provided hints. Nevertheless, a <strong>single</strong> refinement iteration with the hint <strong>improved</strong> performance for all the tested LLMs. </li></ul><ul id="1a51d4b8-0d39-809d-8e57-ea4a77b7c74d" class="bulleted-list"><li style="list-style-type:circle">From the scores, it appears that EDC performance is significantly better on <strong>WebNLG</strong> compared to <strong>REBEL and Wiki-NRE</strong>. <ul id="1a51d4b8-0d39-800f-86a3-e93017208b10" class="bulleted-list"><li style="list-style-type:square">However, the authors observed that EDC was <strong>penalized</strong> despite producing <strong>valid</strong> triplets on the <strong>latter</strong> datasets, because the reference triplets in these datasets are non-exhaustive. <figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a51d4b8-0d39-804b-84a2-cc8afe284ec1"><div style="font-size:1.5em"><span class="icon">🎞️</span></div><div style="width:100%"><p id="1a51d4b8-0d39-80d2-8ece-d8f1371fd95c" class="">For example, given the text in the REBEL dataset, ‘Romany Love is a 1931 British musical film directed by Fred Paul and starring Esmond Knight, Florence McHugh and Roy Travers.’, <strong>EDC</strong> extracts: [‘Romany Love’, ‘cast member’, ‘Esmond Knight’], [‘Romany Love’, ‘cast member’, ‘Florence McHugh’], [‘Romany Love’, ‘cast member’, ‘Roy Travers’], which are all semantically correct, but only <strong>the first triplet</strong> is present in the reference set. </p></div></figure></li></ul><ul id="1a51d4b8-0d39-80a7-baa6-e0be5213a86f" class="bulleted-list"><li style="list-style-type:square">The datasets also contain reference triplets based on information <strong>extraneous</strong> to the text.<figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a51d4b8-0d39-80bd-bf2d-e500fd351af1"><div style="font-size:1.5em"><span class="icon">⚽</span></div><div style="width:100%"><p id="1a51d4b8-0d39-80a2-afa2-fae5b01ac09b" class="">For example, ‘Daniel is an Ethiopian footballer, who currently plays for Hawassa City S.C.’ has a corresponding reference triplet [‘Hawassa City S.C.’, ‘country’, ‘Ethiopia’].</p></div></figure></li></ul><p id="1a51d4b8-0d39-80b7-b265-e0eac7327c54" class="">These issues can be attributed to the distinct methodologies employed in the creation of these datasets.</p></li></ul></li></ul><ul id="1a61d4b8-0d39-8065-b9a3-fe625fa4a796" class="bulleted-list"><li style="list-style-type:disc"><strong>Ablation study on schema retriever. </strong></li></ul><figure id="1a61d4b8-0d39-80ab-b3bc-e7b92dcd9c68" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/t1.png"><img style="width:709.982177734375px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/t1.png"/></a></figure><p id="1a61d4b8-0d39-807e-afcc-e673d206dd50" class="">To evaluate the impact of the relations provided by the schema retriever during refinement, the authors conducted an ablation study with <strong>GPT-3.5-turbo</strong> by <strong>removing</strong> these relations. The results in Table 1 show that ablating the Schema Retriever leads to a <strong>decline</strong> in performance. Qualitatively, they find that the schema retriever helps to find relevant relations that are <strong>challenging</strong> for the LLMs to identify during the OIE stage. </p><figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a61d4b8-0d39-8060-8a28-c8f2e132b8e4"><div style="font-size:1.5em"><span class="icon">🎓</span></div><div style="width:100%"><p id="1a61d4b8-0d39-8072-a361-d4c9d97d7c17" class="">For example, given the text ‘The University of Burgundy in Dijon has 16,800 undergraduate students’, the <strong>LLMs</strong> extract [‘University of Burgundy’, ‘<strong>location</strong>’, ‘Dijon’] during OIE. Although semantically correct, this relation overlooks the more specific relation present in the target schema, namely ‘<strong>campus</strong>’, for denoting university’s location. <strong>The schema retriever</strong> successfully identifies this <strong>finer</strong> relation, enabling the LLMs to adjust their extraction to [‘University of Burgundy’, ‘<strong>campus</strong>’, ‘Dijon’].</p></div></figure><h3 id="1a61d4b8-0d39-80a6-8346-d1f0d981f927" class="">Self Canonicalization</h3><p id="1a61d4b8-0d39-8050-b40e-e4329fcfe4c6" class="">The authors evaluate EDC’s self-canonicalization performance utilizing <strong>GPT-3.5-turbo</strong> for OIE. They <strong>omit refinement</strong> in Self Canonicalization setting, and in subsequent iterations, the self-constructed canonicalized schema becomes the target schema. They conducted a targeted <strong>human evaluation</strong> of knowledge graphs. This evaluation involved <strong>two</strong> independent annotators assessing the reasonableness of triplet extractions from given text without prior knowledge of the system’s details. They observed a high inter-annotator agreement score of <strong>0.94</strong>.</p><figure id="1a61d4b8-0d39-8024-a657-de5f12c5aee4" class="image"><a href="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/t2.png"><img style="width:709.982177734375px" src="Extract,%20Define,%20Canonicalize%20An%20LLM-based%20Framewo%201a41d4b80d39801da86bc8e549a41f23/t2.png"/></a></figure><p id="1a61d4b8-0d39-80fe-bf8f-c98c6cb89830" class="">The evaluation results and schema metrics are summarized in <strong>Table 2</strong>. </p><ul id="1a61d4b8-0d39-80eb-8063-e1c54b2a88e0" class="bulleted-list"><li style="list-style-type:disc">While the <strong>open KG</strong> generated by the OIE stage contains semantically valid triplets, there is a significant degree of <strong>redundancy</strong> within the resultant schema. </li></ul><ul id="1a61d4b8-0d39-801a-be83-da14f8e85ba3" class="bulleted-list"><li style="list-style-type:disc"><strong>EDC</strong> accurately canonicalizes the open KG and yields a schema that is both more concise and less redundant compared to <strong>CESI</strong>. </li></ul><ul id="1a61d4b8-0d39-80c5-8cb6-c61f2f13e713" class="bulleted-list"><li style="list-style-type:disc"><strong>EDC</strong> avoids CESI’s tendency toward <strong>over-generalization.</strong> <figure class="block-color-yellow_background callout" style="white-space:pre-wrap;display:flex" id="1a61d4b8-0d39-8048-be3a-fcb80b96b87b"><div style="font-size:1.5em"><span class="icon">⚰️</span></div><div style="width:100%"><p id="1a61d4b8-0d39-8064-a4ec-e5dfa3f2d065" class="">For example, the authors observed <strong>CESI</strong> inappropriately clusters diverse relations such as ‘place of death’, ‘place of birth’, ‘date of death’, ‘date of birth’, and ‘cause of death’ into a single ‘<strong>date of death</strong>’ category.</p></div></figure></li></ul><h1 id="1a61d4b8-0d39-80b4-994f-db683896e3bd" class="">Conclusion</h1><p id="1a61d4b8-0d39-8051-9e1a-f5c8beea4d42" class="">In this paper, the authors presented EDC, an LLM-based three-phase framework that addresses the problem of KGC by open information extraction followed by post-hoc canonicalization. Experiments show that EDC and EDC+R are able to extract better KGs than specialized trained models when a target schema is available and dynamically create a schema when none is provided. The scalability and versatility of EDC opens up many opportunities for applications: it allows us to automatically extract high-quality KGs from general text using large schemas like Wikidata and even enrich these schemas with newly discovered relations.</p><h1 id="1a41d4b8-0d39-804e-b36e-e15fc784a5c6" class="">Personal Thoughts</h1><ul id="1a61d4b8-0d39-808d-bc14-c82db5dc2935" class="bulleted-list"><li style="list-style-type:disc">Overall, this work is interesting, meaningful and solid to me. Additionally, it has a clear structure and enough examples, so that readers can understand it perfectly. Moreover, it’s open source, which helps a lot to reproduce this work.</li></ul><ul id="1a61d4b8-0d39-8008-9047-e5f86eb81452" class="bulleted-list"><li style="list-style-type:disc">However, this work only focuses on relations, so it would be great to extend it to entities and event types.</li></ul><h1 id="1a61d4b8-0d39-80ee-8579-d085b38cb48c" class="">References </h1><p id="1a61d4b8-0d39-80f2-b1c9-fea3c3eebe0a" class=""><strong><a href="https://arxiv.org/pdf/2404.03868">Paper:</a></strong> Zhang, Bowen, and Harold Soh. &quot;Extract, define, canonicalize: An llm-based framework for knowledge graph construction.&quot; <em>arXiv preprint arXiv:2404.03868</em> (2024).<br/><br/><strong>Code:</strong> <a href="https://github.com/clear-nus/edc">https://github.com/clear-nus/edc</a></p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>